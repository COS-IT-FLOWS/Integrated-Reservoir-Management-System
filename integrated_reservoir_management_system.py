# -*- coding: utf-8 -*-
"""Integrated Reservoir Management System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/149DkoxxdJeEwxVQQqKwr6wLw_ZwAr9tZ

This notebook takes in a Google sheet/csv containing details about dams and their connections in one sheet.
- Column A: Name of dam
- Column B: List of downstream dams of the current dam in a comma separated format.
- Column C: List of spills connections of the current dam in a comma separated format.

The properties of the dams. This includes:
- Coulmn: Catchment Area (sq,km).              [Required]
- Coulmn: Input Runoff (mm)                    [Required]
- Coulmn:Inflow (Mm3)                          [Can be computed]
- Coulmn: Percent Storage (%)	               [Required]
- Coulmn: Gross Storage Capacity (Mm3)	       [Required]
- Coulmn: Maximum Possible Diversion (Mm3/day) [Computed]
- Coulmn: Inflow after diversion (Mm3/day)	   [Computed]
- Coulmn: Expected Spill (Mm3/day)	           [Computed]
- Coulmn: Updated Storage (%)	               [Computed]
- Coulmn:  Spillway Gates Opening	           [Optional]

We also need current runoff and storage % values in the form of a dataframe/list.

The code in the notebook converts the flows and spills between dams into a DAG and performs topological sorting to
determine the order of processing. After this, given the input runoff and current storage %, the code computes the
updated storage, diversion and spill values for every dam and returns it as a dataframe. The code also visualizes the
DAG.

Output:
1. DataFrame containing updated values for storage, diversion and spill for every dam.

Assumptions:
1. The connections between the dams form a DAG. The computations don't make sense if there is a cycle in the graph.
2. The flows and spills are provided in the form of an adjacency list in the second and third columns. We can make the
    columns for spill optional if needed.
3. The column values of the dam properties are expected with a specific column header currently. This can also be
    modified in the future.

TODO:
1. Update this to use data from a single sheet. [Done]
2. Add a temporal component to the delay. Take in distance between dams.
3. Add code to take in inputs as uploaded CSV file, in addition to google sheets.
4. Store the updated dataframe as a CSV or update the sheet with the new values.
5. Test the code with a larger graph and a non-DAG graph.
6. Consider using latitude, longitude values from the dams to derive height and update the plot to reflect these
    dimensions.
"""

import ipywidgets as widgets
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import os
import pandas as pd
from google.colab import drive
from IPython.display import display
from matplotlib.lines import Line2D
import collections


# TODO: This is a work in progress. Finish the details in this class.
class Dam:
    """
    A class to represent a dam and its associated attributes and connections.

    Attributes:
    ----------
    dam_name : str
        The name of the dam.
    connection_list : List[Dam]
        A list of other Dam objects representing the dams connected to this one.
    spill_list : List[Dam]
        A list of other Dam objects where overflow or spill might occur.
    catchment_area : float
        The area that drains into the dam, measured in square kilometers.
    input_runoff : float
        The height of the runoff entering the dam, typically in millimeters.
    inflow : float
        The inflow rate of water entering the dam, usually in Millions of cubic meters.
    storage_percentage : float
        The percentage of the dam's total storage capacity that is currently in use.
    gross_storage_capacity : float
        The total capacity of the dam to hold water, typically in cubic meters.
    max_possible_diversion : float
        The maximum possible amount of water that can be diverted from the dam, in Millions of cubic meters.
    """

    def __init__(self):
        self.name = ''
        self.connection_list = []
        self.spill_list = []
        self.integration_list = []
        self.catchment_area = 0.0
        self.input_runoff = 0.0
        self.inflow = 0.0
        self.storage_pct = 0.0
        self.gross_storage_capacity = 0.0
        self.max_possible_diversion = 0.0
        self.inflow_after_diversion = 0.0
        self.expected_spill = 0.0
        self.updated_storage = 0.0

    def extract_dam_from_row(self, row: pd.Series):
        self.name = row['Dam']
        self.connection_list = extract_adjacency_list_from_string(row['Dam Connection List'])
        self.spill_list = extract_adjacency_list_from_string(row['Dam Spill List'])
        if 'Dam Integration List' in row.columns:
            self.integration_list = extract_adjacency_list_from_string(row['Dam Spill List'])
        self.catchment_area = row['Catchment Area (sq,km)']
        self.input_runoff = row['Catchment Area (sq,km)']
        self.inflow = row['Inflow (Mm3)']
        self.storage_pct = row['Percent Storage(%)']
        self.gross_storage_capacity = row['Gross Storage Capacity (Mm3)']

    def extract_adjacency_list_from_string(self, val: str) -> List[str]:
        if not val:
            return []
        return [v.strip() for v in val.split(',')]


# Read adjacency list column for each dam and return a dictionary of adjacency lists.
def read_adjacency_list(df: pd.DataFrame, column_name: str) -> Dict[str, List[str]]:
    adjacency_list = {}
    for i in range(df.shape[0]):
        neighbor_string = df.iloc[i][column_name]
        neighbors = []
        if neighbor_string and not pd.isna(neighbor_string):
            neighbors = [n.strip() for n in neighbor_string.split(',')]
        adjacency_list[df.index[i]] = neighbors
    return adjacency_list


# Extract adjacency list from a sheet/CSV containing an adjacency matrix.
def extract_adjacency_list_from_matrix(df: pd.DataFrame) -> Dict[str, List[str]]:
    adjacency_matrix = df.values
    # Create an adjacency list
    adjacency_list = {}
    for i in range(adjacency_matrix.shape[0]):
        neighbors = []
        for j in range(adjacency_matrix.shape[1]):
            if adjacency_matrix[i, j] != 0 and i != j:
                neighbors.append(df.columns[j])  # Add 1 to account for the excluded first column
        adjacency_list[df.index[i]] = neighbors  # Add 1 to account for the excluded header row
    return adjacency_list


# Check if a given dictionary of adjacency lists forms a directed acyclic graph.
def is_dag(adjacency_list: Dict[str, List[str]]) -> bool:
    # Initialize all nodes as not visited
    visited = {node: "Not Visited" for node in adjacency_list}

    # Recursive DFS function to check for cycles
    def dfs(node):
        # If the node is in the current path (In Progress), a cycle is found
        if visited[node] == "In Progress":
            return False
        # If the node is already fully visited, no need to visit again
        if visited[node] == "Visited":
            return True

        # Mark the node as in progress
        visited[node] = "In Progress"

        # Visit all neighbors
        for neighbor in adjacency_list.get(node, []):
            if not dfs(neighbor):
                return False

        # Mark the node as fully visited
        visited[node] = "Visited"
        return True

    # Check all nodes in the graph
    for list_node in adjacency_list:
        if visited[list_node] == "Not Visited":
            if not dfs(list_node):
                return False
    return True


# Perform a topological sort of the adjacency list.
def topological_sort(adjacency_list: Dict[str, List[str]]) -> List[str]:
    in_degree = {list_node: 0 for list_node in adjacency_list}
    for node in adjacency_list:
        for neighbor in adjacency_list[node]:
            in_degree[neighbor] += 1

    queue = [list_node for list_node in adjacency_list if in_degree[list_node] == 0]
    result = []

    while queue:
        node = queue.pop(0)
        result.append(node)

        for neighbor in adjacency_list[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
    return result


# Read the dam data from the Google Sheet given and return it as a dataframe.
def read_dam_from_google_sheet(sheet_id: str, gid: str = 0) -> pd.DataFrame:
    if gid:
        dam_data_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={COMBINED_SHEET_GID}'
    else:
        dam_data_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}'

    dam_data_df = pd.read_csv(dam_data_url)
    # Set the column with dam names as the index.
    dam_data_df.set_index('Dam', inplace=True)
    return dam_data_df


# Extract the connections, spill and optionally integration lists from columns in the dataframe.
def extract_dam_dict(dam_matrix_df: pd.DataFrame, connection_column_name: str = 'Dam connection List',
                     spill_column_name: str = 'Dam Spill List',
                     integration_column_name: str = 'Dam Integration List') -> (
        Dict[str: List[str]], Dict[str: List[str]], Dict[str: List[str]]):
    connections_dict = read_adjacency_list(dam_matrix_df, 'Dam Connection List')
    spill_dict = read_adjacency_list(dam_matrix_df, 'Dam Spill List')
    integration_dict = {}
    if 'Dam Integration List' in dam_matrix_df.columns:
        integration_dict = read_adjacency_list(dam_matrix_df, 'Dam Integration List')
    return connections_dict, spill_dict, integration_dict


class NotADagException(Exception):
    pass


def visualize_dams(connections_dict: Dict[str, List[str]], spill_dict: Dict[str, List[str]]):
    # prompt: Visualize the above graph, showing both the adjacency list and spill list as edges on the graph. The spill
    # list edges should have a green color and be drawn with a dotted line. If the spill edge overlaps with a normal edge,
    # draw the spill edge as a curved dotted line.

    # Create a directed graph
    G = nx.DiGraph()

    # Add nodes from the adjacency list
    G.add_nodes_from(connections_dict.keys())

    # Add edges from the adjacency list.
    for node, neighbors in connections_dict.items():
        for neighbor in neighbors:
            G.add_edge(node, neighbor)

    # Add spill edges with green dotted lines.
    for node, neighbors in spill_dict.items():
        for neighbor in neighbors:
            if not G.has_edge(node, neighbor):  # Check for overlapping edges
                G.add_edge(node, neighbor, color='green', style='dotted',
                           connectionstyle='arc3,rad=0.2')  # Curved dotted line

    # Add integration edges with grey dotted lines.
    for node, neighbors in integration_dict.items():
        for neighbor in neighbors:
            if not G.has_edge(node, neighbor):  # Check for overlapping edges
                G.add_edge(node, neighbor, color='grey', style='dotted',
                           connectionstyle='arc3,rad=0.2')  # Curved dotted line

    # Get edge colors and styles
    edge_colors = [G[u][v].get('color', 'black') for u, v in G.edges()]
    edge_styles = [G[u][v].get('style', 'solid') for u, v in G.edges()]

    # Draw the graph with custom edge styles
    pos = nx.spring_layout(G)  # Choose a layout
    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, node_size=1000, node_color='lightblue', font_size=10,
            edge_color=edge_colors, style=edge_styles)

    # Create a legend for edge styles
    legend_elements = [Line2D([0], [0], color='black', label='Normal Edge'),
                       Line2D([0], [0], color='green', linestyle='dotted', label='Spill Edge')]
    plt.legend(handles=legend_elements)

    plt.title("Dam Flow and Spill Graph")
    plt.show()


def update_dam_storage_and_spill(adjacency_list, properties_df, spill_dict, input_runoff_storage_pct_df=pd.DataFrame(),
                                 integration_dict={}):
    updated_properties_df = properties_df.copy(deep=True)
    topological_sorted_dams = topological_sort(adjacency_list)
    processed_integrated_dams = set()

    for dam in topological_sorted_dams:
        print(f'Processing {dam}')
        if dam in processed_integrated_dams:
            print(f'The dam {dam} has already been processed as part of an integrated dam system.')
            continue

        if not input_runoff_storage_pct_df.empty:
            updated_properties_df.loc[dam, 'Input Runoff (mm)'] = input_runoff_storage_pct_df.loc[dam, 'input_runoff']
            updated_properties_df.loc[dam, 'Percent Storage (%)'] = input_runoff_storage_pct_df.loc[dam, 'storage_pct']

        # Process integrated dams together.
        if dam in integration_dict and len(integration_dict[dam]) > 0:
            # Combined properties of the integrated system.
            combined_inflow = updated_properties_df.loc[dam, 'Catchment Area (sq,km)'] * updated_properties_df.loc[
                dam, 'Input Runoff (mm)'] * (10 ** -3)
            combined_storage = (updated_properties_df.loc[dam, 'Percent Storage (%)'] / 100) * \
                               updated_properties_df.loc[dam, 'Gross Storage Capacity (Mm3)']
            combined_max_diversion = updated_properties_df.loc[dam, 'Maximum Possible Diversion (Mm3/day)']
            combined_max_storage = updated_properties_df.loc[dam, 'Gross Storage Capacity (Mm3)']

            # Combined diversion and spill counts and contributions.
            combined_diversion_count = sum(1 if d not in integration_dict[dam] else 0 for d in adjacency_list[dam])
            combined_spill_count = sum(1 if s not in integration_dict[dam] else 0 for s in spill_dict[dam])
            combined_diversions_count_dict = {d: 1.0 for d in adjacency_list[dam] if d not in integration_dict[dam]}
            combined_spill_count_dict = {d: 1.0 for d in spill_dict[dam] if d not in integration_dict[dam]}

            for integrated_neighbor in integration_dict[dam]:
                processed_integrated_dams.add(integrated_neighbor)
                combined_inflow += updated_properties_df.loc[integrated_neighbor, 'Catchment Area (sq,km)'] * \
                                   updated_properties_df.loc[integrated_neighbor, 'Input Runoff (mm)'] * (10 ** -3)
                combined_storage += (updated_properties_df.loc[integrated_neighbor, 'Percent Storage (%)'] / 100) * \
                                    updated_properties_df.loc[integrated_neighbor, 'Gross Storage Capacity (Mm3)']
                combined_max_diversion += updated_properties_df.loc[
                    integrated_neighbor, 'Maximum Possible Diversion (Mm3/day)']
                combined_max_storage += updated_properties_df.loc[integrated_neighbor, 'Gross Storage Capacity (Mm3)']

                for d in adjacency_list[integrated_neighbor]:
                    if d in integration_dict[dam]:
                        continue
                    combined_diversion_count += 1
                    if d not in combined_diversions_count_dict:
                        combined_diversions_count_dict[d] = 1.0
                    else:
                        combined_diversions_count_dict[d] += 1.0
                for d in spill_dict[integrated_neighbor]:
                    if d in integration_dict[dam]:
                        continue
                    combined_spill_count += 1
                    if d not in combined_spill_count_dict:
                        combined_spill_count_dict[d] = 1.0
                    else:
                        combined_spill_count_dict[d] += 1.0

            combined_diversion, combined_spill = combined_inflow, 0.0
            if combined_inflow > combined_max_diversion:
                combined_diversion = combined_max_diversion
                combined_spill = combined_inflow - combined_max_diversion

            combined_new_storage = combined_storage + combined_spill
            if combined_new_storage > combined_max_storage:
                combined_spill += combined_new_storage - combined_max_storage
                combined_new_storage = combined_max_storage

            # I'm making a few assumptions here:
            # 1. The integrated dams have the same heights, hence will share the
            #    storage percentage.
            # 2. The spill for each integrated dam will be the same.
            # These are generalization and not necessarily true. If we have the
            # heights, coordinates and elevation of the dams, we can model this
            # better, but this would get complex. We can add the support for
            # this at a later time.

            combined_pct_storage = (combined_new_storage / combined_max_storage) * 100
            average_spill = combined_spill / (len(integration_dict[dam]) + 1)

            for d in [dam] + integration_dict[dam]:
                updated_properties_df.loc[d, 'Updated Storage (%)'] = combined_pct_storage
                updated_properties_df.loc[d, 'Expected Spill (Mm3/day)'] = average_spill
                print(
                    f'Integrated dam processing: Updated properties for {d}: Updated Storage: {updated_properties_df.loc[d, "Updated Storage (%)"]}, Expected Spill: {updated_properties_df.loc[d, "Expected Spill (Mm3/day)"]}')

            if combined_diversion_count > 0:
                diversion_per_neighbor = combined_diversion / combined_diversion_count
                for neighbor in combined_diversions_count_dict:
                    if neighbor in integration_dict[dam]:
                        continue
                    updated_properties_df.loc[neighbor, 'Inflow (Mm3)'] += diversion_per_neighbor * \
                                                                           combined_diversions_count_dict[neighbor]
                    print(
                        f'Integrated dam processing: Updated properties for neighbor {neighbor} through diversion: Inflow: {updated_properties_df.loc[neighbor, "Inflow (Mm3)"]}')

            if combined_spill_count > 0:
                spill_per_neighbor = combined_spill / combined_spill_count
                for neighbor in combined_spill_count_dict:
                    if neighbor in integration_dict[dam]:
                        continue
                    updated_properties_df.loc[neighbor, 'Inflow (Mm3)'] += spill_per_neighbor * \
                                                                           combined_spill_count_dict[neighbor]
                    print(
                        f'Integrated dam processing: Updated properties for neighbor {neighbor} through spill: Inflow: {updated_properties_df.loc[neighbor, "Inflow (Mm3)"]}')
        # Process dams that are not integrated.
        else:
            catchment_area = updated_properties_df.loc[dam, 'Catchment Area (sq,km)']
            current_inflow = catchment_area * updated_properties_df.loc[dam, 'Input Runoff (mm)'] * (10 ** -3)

            # TODO: Fix this logic.
            # This should be updated_properties_df.loc[dam, 'Inflow (Mm3)'] += current_inflow
            # Otherwise the updated inflow from spills and diversions are not considered.
            updated_properties_df.loc[dam, 'Inflow (Mm3)'] += current_inflow
            current_storage = (updated_properties_df.loc[dam, 'Percent Storage (%)'] / 100) * updated_properties_df.loc[
                dam, 'Gross Storage Capacity (Mm3)']
            max_diversion = updated_properties_df.loc[dam, 'Maximum Possible Diversion (Mm3/day)']
            max_storage = updated_properties_df.loc[dam, 'Gross Storage Capacity (Mm3)']

            diversion, spill = current_inflow, 0.0
            if current_inflow > max_diversion:
                diversion = max_diversion
                spill = current_inflow - max_diversion

            new_storage = current_storage + spill
            if new_storage > max_storage:
                spill += new_storage - max_storage
                new_storage = max_storage

            updated_properties_df.loc[dam, 'Updated Storage (%)'] = (new_storage / updated_properties_df.loc[
                dam, 'Gross Storage Capacity (Mm3)']) * 100
            updated_properties_df.loc[dam, 'Expected Spill (Mm3/day)'] = spill

            print(
                f'Updated properties: Updated Storage: {updated_properties_df.loc[dam, "Updated Storage (%)"]}, Expected Spill: {updated_properties_df.loc[dam, "Expected Spill (Mm3/day)"]}')

            diversion_count, spill_count = len(adjacency_list[dam]), len(spill_dict[dam])
            if diversion_count > 0:
                diversion_per_neighbor = diversion / diversion_count
                for neighbor in adjacency_list[dam]:
                    updated_properties_df.loc[neighbor, 'Inflow (Mm3)'] += diversion_per_neighbor
                    print(
                        f'Updated properties for neighbor {neighbor} through diversion: Inflow: {updated_properties_df.loc[neighbor, "Inflow (Mm3)"]}')
            if spill_count > 0:
                spill_per_neighbor = spill / spill_count
                for neighbor in spill_dict[dam]:
                    updated_properties_df.loc[neighbor, 'Inflow (Mm3)'] += spill_per_neighbor
                    print(
                        f'Updated properties for neighbor {neighbor} through spill: Inflow: {updated_properties_df.loc[neighbor, "Inflow (Mm3)"]}')
    print('\n')
    return updated_properties_df


def main(self, args):
    # Replace with your Google Sheet ID.
    SHEET_ID = '177HyFTJhKtVKN1JRnKwV2wOhoUgyMr9A3490UIQi1Tg'
    # Replace with your Google Sheet GID if it's not the first sheet.
    COMBINED_SHEET_GID = '1158329665'

    dam_data_df = read_dam_from_google_sheet(SHEET_ID, COMBINED_SHEET_GID)
    connections_dict, spill_dict, integration_dict = extract_dam_dict(dam_data_df)
    dam_properties_df = dam_data_df.drop(
        columns=['Dam Connection List', 'Dam Spill List', 'Dam Integration List']).fillna(0)

    # Check whether the connections dict forms a DAG.
    if not is_dag(connections_dict):
        raise NotADagException("The graph is not a DAG.")

    # Perform a topological sort of the dams.
    sorted_dams = topological_sort(connections_dict)

    input_runoff_storage_df = pd.DataFrame({
        'input_runoff': [100, 100, 100, 100, 100, 100, 100, 100],
        'storage_pct': [80, 80, 80, 80, 80, 80, 80, 80]
    },
        index=['Dam 1', 'Dam 2', 'Dam 3', 'Dam 4', 'Dam 5', 'Dam 6', 'Dam 7', 'Dam 8'])

    updated_dam_properties_df = update_dam_storage_and_spill(connections_dict, dam_properties_df, spill_dict,
                                                             input_runoff_storage_df)

    updated_dam_properties_integrated_df = update_dam_storage_and_spill(connections_dict, dam_properties_df, spill_dict,
                                                                        input_runoff_storage_df, integration_dict)

    # input_runoff_storage_df_2 = pd.DataFrame({
    #     'input_runoff': [200, 200, 200, 200, 200, 200, 200, 200],
    #     'storage_pct': [50, 50, 50, 50, 50, 50, 50, 50]
    # },
    #     index=['Dam 1', 'Dam 2', 'Dam 3', 'Dam 4', 'Dam 5', 'Dam 6', 'Dam 7', 'Dam 8'])
    #
    # updated_dam_properties_df_2 = update_dam_storage_and_spill(connections_dict, dam_properties_df, spill_dict,
    #                                                     input_runoff_storage_df_2, integration_dict)
    # updated_dam_properties_df_2

    print(f'Updated dam properties.\n')
    print(updated_dam_properties_integrated_df)
